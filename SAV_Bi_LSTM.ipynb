{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pyvi import ViTokenizer\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, SimpleRNN, Dropout, Convolution1D, Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_excel('Data/data_aivivn.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16087 entries, 0 to 16086\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Label     16087 non-null  int64 \n",
      " 1   Sentence  16087 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 251.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8882\n",
       "1    7205\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_sentiment = pd.value_counts(train['Label'])\n",
    "count_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(text):\n",
    "    \n",
    "    # Chuyển thành chữ thường\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Chuan hoa tieng viet, tieng anh, thuat ngu\n",
    "    replace_list={\n",
    "        'òa': 'oà', 'óa': 'oá', 'ỏa': 'oả', 'õa': 'oã', 'ọa': 'oạ', 'òe': 'oè', 'óe': 'oé','ỏe': 'oẻ',\n",
    "    'õe': 'oẽ', 'ọe': 'oẹ', 'ùy': 'uỳ', 'úy': 'uý', 'ủy': 'uỷ', 'ũy': 'uỹ','ụy': 'uỵ', 'uả': 'ủa',\n",
    "    'ả': 'ả', 'ố': 'ố', 'u´': 'ố','ỗ': 'ỗ', 'ồ': 'ồ', 'ổ': 'ổ', 'ấ': 'ấ', 'ẫ': 'ẫ', 'ẩ': 'ẩ',\n",
    "    'ầ': 'ầ', 'ỏ': 'ỏ', 'ề': 'ề','ễ': 'ễ', 'ắ': 'ắ', 'ủ': 'ủ', 'ế': 'ế', 'ở': 'ở', 'ỉ': 'ỉ',\n",
    "    'ẻ': 'ẻ', 'àk': u' à ','aˋ': 'à', 'iˋ': 'ì', 'ă´': 'ắ','ử': 'ử', 'e˜': 'ẽ', 'y˜': 'ỹ', 'a´': 'á',\n",
    "    #Quy các icon về 2 loại emoj: Tích cực hoặc tiêu cực\n",
    "    \"👹\": \"nagative\", \"👻\": \"positive\", \"💃\": \"positive\",'🤙': ' positive ', '👍': ' positive ',\n",
    "    \"💄\": \"positive\", \"💎\": \"positive\", \"💩\": \"positive\",\"😕\": \"nagative\", \"😱\": \"nagative\", \"😸\": \"positive\",\n",
    "    \"😾\": \"nagative\", \"🚫\": \"nagative\",  \"🤬\": \"nagative\",\"🧚\": \"positive\", \"🧡\": \"positive\",'🐶':' positive ',\n",
    "    '👎': ' nagative ', '😣': ' nagative ','✨': ' positive ', '❣': ' positive ','☀': ' positive ',\n",
    "    '♥': ' positive ', '🤩': ' positive ', 'like': ' positive ', '💌': ' positive ',\n",
    "    '🤣': ' positive ', '🖤': ' positive ', '🤤': ' positive ', ':(': ' nagative ', '😢': ' nagative ',\n",
    "    '❤': ' positive ', '😍': ' positive ', '😘': ' positive ', '😪': ' nagative ', '😊': ' positive ',\n",
    "    '?': ' ? ', '😁': ' positive ', '💖': ' positive ', '😟': ' nagative ', '😭': ' nagative ',\n",
    "    '💯': ' positive ', '💗': ' positive ', '♡': ' positive ', '💜': ' positive ', '🤗': ' positive ',\n",
    "    '^^': ' positive ', '😨': ' nagative ', '☺': ' positive ', '💋': ' positive ', '👌': ' positive ',\n",
    "    '😖': ' nagative ', '😀': ' positive ', ':((': ' nagative ', '😡': ' nagative ', '😠': ' nagative ',\n",
    "    '😒': ' nagative ', '🙂': ' positive ', '😏': ' nagative ', '😝': ' positive ', '😄': ' positive ',\n",
    "    '😙': ' positive ', '😤': ' nagative ', '😎': ' positive ', '😆': ' positive ', '💚': ' positive ',\n",
    "    '✌': ' positive ', '💕': ' positive ', '😞': ' nagative ', '😓': ' nagative ', '️🆗️': ' positive ',\n",
    "    '😉': ' positive ', '😂': ' positive ', ':v': '  positive ', '=))': '  positive ', '😋': ' positive ',\n",
    "    '💓': ' positive ', '😐': ' nagative ', ':3': ' positive ', '😫': ' nagative ', '😥': ' nagative ',\n",
    "    '😃': ' positive ', '😬': ' 😬 ', '😌': ' 😌 ', '💛': ' positive ', '🤝': ' positive ', '🎈': ' positive ',\n",
    "    '😗': ' positive ', '🤔': ' nagative ', '😑': ' nagative ', '🔥': ' nagative ', '🙏': ' nagative ',\n",
    "    '🆗': ' positive ', '😻': ' positive ', '💙': ' positive ', '💟': ' positive ',\n",
    "    '😚': ' positive ', '❌': ' nagative ', '👏': ' positive ', ';)': ' positive ', '<3': ' positive ',\n",
    "    '🌝': ' positive ',  '🌷': ' positive ', '🌸': ' positive ', '🌺': ' positive ',\n",
    "    '🌼': ' positive ', '🍓': ' positive ', '🐅': ' positive ', '🐾': ' positive ', '👉': ' positive ',\n",
    "    '💐': ' positive ', '💞': ' positive ', '💥': ' positive ', '💪': ' positive ',\n",
    "    '💰': ' positive ',  '😇': ' positive ', '😛': ' positive ', '😜': ' positive ',\n",
    "    '🙃': ' positive ', '🤑': ' positive ', '🤪': ' positive ','☹': ' nagative ',  '💀': ' nagative ',\n",
    "    '😔': ' nagative ', '😧': ' nagative ', '😩': ' nagative ', '😰': ' nagative ', '😳': ' nagative ',\n",
    "    '😵': ' nagative ', '😶': ' nagative ', '🙁': ' nagative ',\n",
    "    #Chuẩn hóa 1 số sentiment words/English words\n",
    "    ':))': '  positive ', ':)': ' positive ', 'ô kêi': ' ok ', 'okie': ' ok ', ' o kê ': ' ok ',\n",
    "    'okey': ' ok ', 'ôkê': ' ok ', ' oki ': ' ok ', ' oke ':  ' ok ',' okay':' ok ','okê':' ok ',\n",
    "    ' tks ': u' cám ơn ', 'thks': u' cám ơn ', 'thanks': u' cám ơn ', 'ths': u' cám ơn ', 'thank': u' cám ơn ', 'cam on':u'cám ơn',u'cảm ơn':'cám ơn',\n",
    "    ' not ': u' không ',' khoong ': ' không ', u' kg ': u' không ','ko ': 'không', ' k ': u' không ',' kh ':u' không ',' kô ':u' không ','hok':u' không ',' kp ': u' không phải ',u' kô ': u' không ', ' ko ': u' không ', u' ko ': u' không ', u' k ': u' không ', 'khong': u' không ', u' hok ': u' không ',' k ':u' không ',u'chẳng':u'không',u'đéo':u'không',\n",
    "    'he he': ' positive ','hehe': ' positive ','hihi': ' positive ', 'haha': ' positive ', 'hjhj': ' positive ',\n",
    "    ' lol ': ' negative ',' cc ': ' negative ','cute': u' dễ thương ','huhu': ' negative ', ' vs ': u' với ', 'wa': ' quá ', 'wá': u' quá', ' j ': u' gì ', '“': ' ',\n",
    "    ' sz ': u' cỡ ', 'size': u' cỡ ', u' đx ': u' được ', 'dk': u' được ', 'dc': u' được ', 'đk': u' được ',\n",
    "    'đc': u' được ','authentic': u' chuẩn chính hãng ',u' aut ': u' chuẩn chính hãng ', u' auth ': u' chuẩn chính hãng ', 'thick': u' positive ', 'store': u' cửa hàng ',\n",
    "    'shop': u' cửa hàng ', 'sp': u' sản phẩm ', 'gud': u' tốt ','god': u' tốt ','wel done':' tốt ', 'good': u' tốt ', 'gút': u' tốt ',\n",
    "    'sấu': u' xấu ','gut': u' tốt ', u' tot ': u' tốt ', u' nice ': u' tốt ', 'perfect': 'rất tốt', 'bt': u' bình thường ',\n",
    "    'time': u' thời gian ', 'qá': u' quá ', u' ship ': u' giao hàng ', u' m ': u' mình ', u' mik ': u' mình ',\n",
    "    'ể': 'ể', 'product': 'sản phẩm', 'quality': 'chất lượng','chat':' chất ', 'excelent': 'hoàn hảo', 'bad': 'tệ','fresh': ' tươi ','sad': ' tệ ',\n",
    "    'date': u' hạn sử dụng ', 'hsd': u' hạn sử dụng ','quickly': u' nhanh ', 'quick': u' nhanh ','fast': u' nhanh ','delivery': u' giao hàng ',u' síp ': u' giao hàng ',\n",
    "    'beautiful': u' đẹp tuyệt vời ', u' tl ': u' trả lời ', u' r ': u' rồi ', u' shopE ': u' cửa hàng ',u' order ': u' đặt hàng ',\n",
    "    'chất lg': u' chất lượng ',u' sd ': u' sử dụng ',u' dt ': u' điện thoại ',u' nt ': u' nhắn tin ',u' tl ': u' trả lời ',u' sài ': u' xài ',u'bjo':u' bao giờ ',\n",
    "    'thik': u' thích ',u' sop ': u' cửa hàng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' rất ', ' quả ng ': ' quảng ',\n",
    "    'dep': u' đẹp ',u'xầu':u'xấu',u' xau ': u' xấu ','delicious': u' ngon ', u'hàg': u' hàng ', u' qủa': u' quả', \n",
    "    ' iu ': u' yêu ','fake': u' giả mạo ', 'trl': 'trả lời', '><': u' positive ', \n",
    "    ' por ': u' tệ ',' poor ': u' tệ ', 'ib':u' nhắn tin ', 'rep':u' trả lời ',u'fback':' feedback ','fedback':' feedback ',\n",
    "    'qc':u'quảng cáo',u' éo ':' negative ',' bik ':' biết ',\n",
    "    #dưới 3* quy về 1*, trên 3* quy về 5*\n",
    "    '6 sao': ' 5star ','6 star': ' 5star ', '5star': ' 5star ','5 sao': ' 5star ','5sao': ' 5star ', '5 sao': '5star',\n",
    "    'starstarstarstarstar': ' 5star ', '1 sao': ' 1star ', '1sao': ' 1star ','2 sao':' 1star ','2sao':' 1star ', '3 sao':'5star','4 sao':'5star','1 sao':'1star',\n",
    "    '2 starstar':' 1star ','1star': ' 1star ', '0 sao': ' 1star ', '0star': ' 1star ','1*':' 1star ', '2*':' 1star ','3*':' 5star ','4*':' 5star ','5*':' 5star '\n",
    "    }\n",
    "    for k, v in replace_list.items():\n",
    "        text = text.replace(k, v)\n",
    "        \n",
    "    # Xóa icon\n",
    "    text = re.sub(r\"\\W\", \" \",text)\n",
    "    \n",
    "    # Xóa dấu câu\n",
    "    text = re.sub('['+string.punctuation+']', ' ', text)\n",
    "    \n",
    "    # Tokenizer\n",
    "    text = ViTokenizer.tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_Sentence = []\n",
    "for i in range(train.shape[0]):\n",
    "    New_Sentence.append(pre_processing(train['Sentence'][i]))\n",
    "train['New_Sentence'] = New_Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>New_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"dung dc sp tot cam on shop đóng gói sản phẩm ...</td>\n",
       "      <td>dung được sản_phẩm tốt cám_ơn cửa_hàng đóng_gó...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>\" chất lượng sản phẩm tuyệt vời . son mịn nhưn...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời son mịn nhưng kh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>\" chất lượng sản phẩm tuyệt vời nhưng k có hộp...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời nhưng không có h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>\":(( mình hơi thất vọng 1 chút vì mình đã kỳ v...</td>\n",
       "      <td>nagative mình hơi thất_vọng 1 chút vì mình đã ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>\"lần trước mình mua áo gió màu hồng rất ok mà ...</td>\n",
       "      <td>lần trước mình mua áo_gió màu hồng rất ok mà đ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                           Sentence  \\\n",
       "0      0  \"dung dc sp tot cam on shop đóng gói sản phẩm ...   \n",
       "1      0  \" chất lượng sản phẩm tuyệt vời . son mịn nhưn...   \n",
       "2      0  \" chất lượng sản phẩm tuyệt vời nhưng k có hộp...   \n",
       "3      1  \":(( mình hơi thất vọng 1 chút vì mình đã kỳ v...   \n",
       "4      1  \"lần trước mình mua áo gió màu hồng rất ok mà ...   \n",
       "\n",
       "                                        New_Sentence  \n",
       "0  dung được sản_phẩm tốt cám_ơn cửa_hàng đóng_gó...  \n",
       "1  chất_lượng sản_phẩm tuyệt_vời son mịn nhưng kh...  \n",
       "2  chất_lượng sản_phẩm tuyệt_vời nhưng không có h...  \n",
       "3  nagative mình hơi thất_vọng 1 chút vì mình đã ...  \n",
       "4  lần trước mình mua áo_gió màu hồng rất ok mà đ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(train.loc[:,'New_Sentence'].values, train.loc[:,'Label'].values, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10295,)\n",
      "(2574,)\n",
      "(3218,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token and Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 64\n",
    "max_length = 100\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo Tokenizer\n",
    "tokenizer = Tokenizer(num_words= vocab_size, oov_token= oov_tok)\n",
    "\n",
    "# Đưa từ vào tokenizer để tạo từ điển\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "# Biến các câu train thành sequences\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "# Padding cho các train sequences\n",
    "x_train = pad_sequences(x_train, maxlen= max_length, padding=padding_type, truncating= trunc_type)\n",
    "\n",
    "# Biến các câu val thành sequences tương ứng\n",
    "x_val = tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "x_val = pad_sequences(x_val, maxlen= max_length, padding=padding_type, truncating= trunc_type)\n",
    "\n",
    "# Biến các câu test thành sequences tương ứng\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_test = pad_sequences(X_test, maxlen= max_length, padding=padding_type, truncating= trunc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buil Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 64)           640000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 665,889\n",
      "Trainable params: 665,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=x_train.shape[1]))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "322/322 [==============================] - 16s 49ms/step - loss: 0.3000 - accuracy: 0.8645 - val_loss: 0.1716 - val_accuracy: 0.9332\n",
      "Epoch 2/4\n",
      "322/322 [==============================] - 16s 49ms/step - loss: 0.1335 - accuracy: 0.9490 - val_loss: 0.1731 - val_accuracy: 0.9308\n",
      "Epoch 3/4\n",
      "322/322 [==============================] - 15s 47ms/step - loss: 0.0866 - accuracy: 0.9694 - val_loss: 0.1891 - val_accuracy: 0.9293\n",
      "Epoch 4/4\n",
      "322/322 [==============================] - 15s 46ms/step - loss: 0.0594 - accuracy: 0.9814 - val_loss: 0.2221 - val_accuracy: 0.9289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2757c51eb38>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=4, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 1s 9ms/step - loss: 0.2273 - accuracy: 0.9313\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiêu cực\n"
     ]
    }
   ],
   "source": [
    "t = ['thế thì chịu rồi']\n",
    "t = tokenizer.texts_to_sequences(t)\n",
    "t = pad_sequences(t, maxlen= max_length, padding=padding_type, truncating= trunc_type)\n",
    "result = model.predict(t)\n",
    "if result <=0.5: \n",
    "    print('tích cực')\n",
    "else:\n",
    "    print('tiêu cực')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
